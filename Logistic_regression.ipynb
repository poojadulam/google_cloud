{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4662a3-0889-4f2b-b6f1-8850fb171b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'valiant-ocean-401219'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462579cf-e695-47c7-94d2-19ec3cdd51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-east1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95283b43-1fa2-453c-b3ab-8fdb653dd413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97225d11-6241-40e6-a583-57c5bb8759a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs = storage.Client(project = PROJECT_ID)\n",
    "bq = bigquery.Client(project = PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a2dd17-fb9c-40d5-840a-8889086067e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ad3b3f-6b61-4b6d-954b-3e2386d98929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket already exist: valiant-ocean-401219\n"
     ]
    }
   ],
   "source": [
    "if not gcs.lookup_bucket(BUCKET):\n",
    "    bucketDef = gcs.bucket(BUCKET)\n",
    "    bucket = gcs.create_bucket(bucketDef, project=PROJECT_ID, location=REGION)\n",
    "    print(f'Created Bucket: {gcs.lookup_bucket(BUCKET).name}')\n",
    "else:\n",
    "    bucketDef = gcs.bucket(BUCKET)\n",
    "    print(f'Bucket already exist: {bucketDef.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32d5ef11-02d7-4b6d-9279-c53ab67bcfd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21381076265-compute@developer.gserviceaccount.com'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f782fc8-ffba-4b70-85c9-b24941d6d2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROLE\n",
      "roles/editor\n",
      "roles/run.admin\n",
      "roles/storage.objectAdmin\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects get-iam-policy $PROJECT_ID --filter=\"bindings.members:$SERVICE_ACCOUNT\" --format='table(bindings.role)' --flatten=\"bindings[].members\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4484b121-2ad7-4471-8de4-7e299a3eeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86421a84-9a08-41c6-9809-1dd38e24bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-pipeline-components -U -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd483fc-6332-404d-9f46-820e447e5c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.33.1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "aiplatform.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5ff9dad-a822-4840-9582-a7d1ad75027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e3371c6-07ea-491f-bee6-9de8f830d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "bq = bigquery.Client(project = PROJECT_ID)\n",
    "gcs = storage.Client(project = PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6407a1c6-cfc8-499b-970c-70bb11ee6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = '01'\n",
    "SERIES = '01'\n",
    "\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = 'weather'\n",
    "BQ_TABLE = 'weatherData_noNull_prepped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e809d458-ef6d-4f19-9baf-1d0c94abea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather\n"
     ]
    }
   ],
   "source": [
    "datasets = list(bq.list_datasets())\n",
    "for d in datasets:\n",
    "    print(d.dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2dc4d711-3eb1-4e53-9e50-934bdae55214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Formatted_Date</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Precip_Type</th>\n",
       "      <th>Temperature__C_</th>\n",
       "      <th>Apparent_Temperature__C_</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind_Speed__km_h_</th>\n",
       "      <th>Wind_Bearing__degrees_</th>\n",
       "      <th>Visibility__km_</th>\n",
       "      <th>Loud_Cover</th>\n",
       "      <th>Pressure__millibars_</th>\n",
       "      <th>Daily_Summary</th>\n",
       "      <th>row_ID</th>\n",
       "      <th>splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-03-21 14:00:00+00:00</td>\n",
       "      <td>Dry</td>\n",
       "      <td>rain</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.21</td>\n",
       "      <td>14.4900</td>\n",
       "      <td>320</td>\n",
       "      <td>9.9820</td>\n",
       "      <td>0</td>\n",
       "      <td>1032.20</td>\n",
       "      <td>Clear throughout the day.</td>\n",
       "      <td>4944af1e-4627-44b4-ba42-a4e99ade82bc</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-03-17 16:00:00+00:00</td>\n",
       "      <td>Dry</td>\n",
       "      <td>rain</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.24</td>\n",
       "      <td>12.8800</td>\n",
       "      <td>190</td>\n",
       "      <td>9.9820</td>\n",
       "      <td>0</td>\n",
       "      <td>1021.10</td>\n",
       "      <td>Partly cloudy in the morning.</td>\n",
       "      <td>b183c577-863c-4025-8436-6c50b50afc18</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-03-17 10:00:00+00:00</td>\n",
       "      <td>Dry</td>\n",
       "      <td>rain</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.26</td>\n",
       "      <td>14.4900</td>\n",
       "      <td>200</td>\n",
       "      <td>9.9820</td>\n",
       "      <td>0</td>\n",
       "      <td>1025.20</td>\n",
       "      <td>Partly cloudy in the morning.</td>\n",
       "      <td>2f6f438e-2e75-464c-a1e9-fa827f3f874e</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-08-20 08:00:00+00:00</td>\n",
       "      <td>Dry</td>\n",
       "      <td>rain</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>28.677778</td>\n",
       "      <td>0.29</td>\n",
       "      <td>4.4919</td>\n",
       "      <td>355</td>\n",
       "      <td>9.9820</td>\n",
       "      <td>0</td>\n",
       "      <td>1022.68</td>\n",
       "      <td>Clear throughout the day.</td>\n",
       "      <td>aa31c8ae-7a6d-4e89-b05b-fce6d303e122</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-07-22 11:00:00+00:00</td>\n",
       "      <td>Dry</td>\n",
       "      <td>rain</td>\n",
       "      <td>38.750000</td>\n",
       "      <td>36.622222</td>\n",
       "      <td>0.15</td>\n",
       "      <td>21.8799</td>\n",
       "      <td>230</td>\n",
       "      <td>9.9820</td>\n",
       "      <td>0</td>\n",
       "      <td>1008.74</td>\n",
       "      <td>Partly cloudy starting in the afternoon.</td>\n",
       "      <td>8051d38c-4391-4eee-addb-b3b84ce185f5</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95931</th>\n",
       "      <td>2009-04-28 12:00:00+00:00</td>\n",
       "      <td>Breezy and Partly Cloudy</td>\n",
       "      <td>rain</td>\n",
       "      <td>21.022222</td>\n",
       "      <td>21.022222</td>\n",
       "      <td>0.27</td>\n",
       "      <td>35.7742</td>\n",
       "      <td>140</td>\n",
       "      <td>10.3684</td>\n",
       "      <td>0</td>\n",
       "      <td>1011.27</td>\n",
       "      <td>Breezy starting in the morning continuing unti...</td>\n",
       "      <td>6a5beb8a-d4cc-4a82-a949-696d4a10e272</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95932</th>\n",
       "      <td>2016-08-10 17:00:00+00:00</td>\n",
       "      <td>Breezy and Partly Cloudy</td>\n",
       "      <td>rain</td>\n",
       "      <td>19.772222</td>\n",
       "      <td>19.772222</td>\n",
       "      <td>0.64</td>\n",
       "      <td>31.4433</td>\n",
       "      <td>310</td>\n",
       "      <td>9.9820</td>\n",
       "      <td>0</td>\n",
       "      <td>1011.06</td>\n",
       "      <td>Partly cloudy throughout the day.</td>\n",
       "      <td>c215fb20-b164-4e98-bf33-a24ded6f3d91</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95933</th>\n",
       "      <td>2015-05-10 13:00:00+00:00</td>\n",
       "      <td>Breezy and Partly Cloudy</td>\n",
       "      <td>rain</td>\n",
       "      <td>23.772222</td>\n",
       "      <td>23.772222</td>\n",
       "      <td>0.31</td>\n",
       "      <td>34.6633</td>\n",
       "      <td>311</td>\n",
       "      <td>16.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1018.48</td>\n",
       "      <td>Mostly cloudy until night and breezy starting ...</td>\n",
       "      <td>c28e9a77-283d-4828-91ab-411a6c89c03d</td>\n",
       "      <td>TEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95934</th>\n",
       "      <td>2012-08-26 17:00:00+00:00</td>\n",
       "      <td>Breezy and Partly Cloudy</td>\n",
       "      <td>rain</td>\n",
       "      <td>23.772222</td>\n",
       "      <td>23.772222</td>\n",
       "      <td>0.42</td>\n",
       "      <td>33.0533</td>\n",
       "      <td>319</td>\n",
       "      <td>9.9820</td>\n",
       "      <td>0</td>\n",
       "      <td>1008.17</td>\n",
       "      <td>Breezy starting in the afternoon continuing un...</td>\n",
       "      <td>b30dc9f8-851e-41af-a01e-1a044369ebed</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95935</th>\n",
       "      <td>2007-01-29 12:00:00+00:00</td>\n",
       "      <td>Dangerously Windy and Partly Cloudy</td>\n",
       "      <td>rain</td>\n",
       "      <td>8.944444</td>\n",
       "      <td>3.483333</td>\n",
       "      <td>0.49</td>\n",
       "      <td>63.8526</td>\n",
       "      <td>307</td>\n",
       "      <td>11.4471</td>\n",
       "      <td>0</td>\n",
       "      <td>1009.05</td>\n",
       "      <td>Mostly cloudy throughout the day and windy sta...</td>\n",
       "      <td>d4660a09-e3c8-4e72-a1ef-bc2f09540de0</td>\n",
       "      <td>TRAIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95936 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Formatted_Date                              Summary  \\\n",
       "0     2012-03-21 14:00:00+00:00                                  Dry   \n",
       "1     2012-03-17 16:00:00+00:00                                  Dry   \n",
       "2     2012-03-17 10:00:00+00:00                                  Dry   \n",
       "3     2012-08-20 08:00:00+00:00                                  Dry   \n",
       "4     2007-07-22 11:00:00+00:00                                  Dry   \n",
       "...                         ...                                  ...   \n",
       "95931 2009-04-28 12:00:00+00:00             Breezy and Partly Cloudy   \n",
       "95932 2016-08-10 17:00:00+00:00             Breezy and Partly Cloudy   \n",
       "95933 2015-05-10 13:00:00+00:00             Breezy and Partly Cloudy   \n",
       "95934 2012-08-26 17:00:00+00:00             Breezy and Partly Cloudy   \n",
       "95935 2007-01-29 12:00:00+00:00  Dangerously Windy and Partly Cloudy   \n",
       "\n",
       "      Precip_Type  Temperature__C_  Apparent_Temperature__C_  Humidity  \\\n",
       "0            rain        20.000000                 20.000000      0.21   \n",
       "1            rain        20.000000                 20.000000      0.24   \n",
       "2            rain        20.000000                 20.000000      0.26   \n",
       "3            rain        30.000000                 28.677778      0.29   \n",
       "4            rain        38.750000                 36.622222      0.15   \n",
       "...           ...              ...                       ...       ...   \n",
       "95931        rain        21.022222                 21.022222      0.27   \n",
       "95932        rain        19.772222                 19.772222      0.64   \n",
       "95933        rain        23.772222                 23.772222      0.31   \n",
       "95934        rain        23.772222                 23.772222      0.42   \n",
       "95935        rain         8.944444                  3.483333      0.49   \n",
       "\n",
       "       Wind_Speed__km_h_  Wind_Bearing__degrees_  Visibility__km_  Loud_Cover  \\\n",
       "0                14.4900                     320           9.9820           0   \n",
       "1                12.8800                     190           9.9820           0   \n",
       "2                14.4900                     200           9.9820           0   \n",
       "3                 4.4919                     355           9.9820           0   \n",
       "4                21.8799                     230           9.9820           0   \n",
       "...                  ...                     ...              ...         ...   \n",
       "95931            35.7742                     140          10.3684           0   \n",
       "95932            31.4433                     310           9.9820           0   \n",
       "95933            34.6633                     311          16.1000           0   \n",
       "95934            33.0533                     319           9.9820           0   \n",
       "95935            63.8526                     307          11.4471           0   \n",
       "\n",
       "       Pressure__millibars_  \\\n",
       "0                   1032.20   \n",
       "1                   1021.10   \n",
       "2                   1025.20   \n",
       "3                   1022.68   \n",
       "4                   1008.74   \n",
       "...                     ...   \n",
       "95931               1011.27   \n",
       "95932               1011.06   \n",
       "95933               1018.48   \n",
       "95934               1008.17   \n",
       "95935               1009.05   \n",
       "\n",
       "                                           Daily_Summary  \\\n",
       "0                              Clear throughout the day.   \n",
       "1                          Partly cloudy in the morning.   \n",
       "2                          Partly cloudy in the morning.   \n",
       "3                              Clear throughout the day.   \n",
       "4               Partly cloudy starting in the afternoon.   \n",
       "...                                                  ...   \n",
       "95931  Breezy starting in the morning continuing unti...   \n",
       "95932                  Partly cloudy throughout the day.   \n",
       "95933  Mostly cloudy until night and breezy starting ...   \n",
       "95934  Breezy starting in the afternoon continuing un...   \n",
       "95935  Mostly cloudy throughout the day and windy sta...   \n",
       "\n",
       "                                     row_ID splits  \n",
       "0      4944af1e-4627-44b4-ba42-a4e99ade82bc  TRAIN  \n",
       "1      b183c577-863c-4025-8436-6c50b50afc18   TEST  \n",
       "2      2f6f438e-2e75-464c-a1e9-fa827f3f874e  TRAIN  \n",
       "3      aa31c8ae-7a6d-4e89-b05b-fce6d303e122  TRAIN  \n",
       "4      8051d38c-4391-4eee-addb-b3b84ce185f5  TRAIN  \n",
       "...                                     ...    ...  \n",
       "95931  6a5beb8a-d4cc-4a82-a949-696d4a10e272  TRAIN  \n",
       "95932  c215fb20-b164-4e98-bf33-a24ded6f3d91  TRAIN  \n",
       "95933  c28e9a77-283d-4828-91ab-411a6c89c03d   TEST  \n",
       "95934  b30dc9f8-851e-41af-a01e-1a044369ebed  TRAIN  \n",
       "95935  d4660a09-e3c8-4e72-a1ef-bc2f09540de0  TRAIN  \n",
       "\n",
       "[95936 rows x 14 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}` TABLESAMPLE SYSTEM (1 PERCENT)\n",
    "#LIMIT 5\n",
    "\"\"\"\n",
    "bq.query(query = query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7d99365-84e0-46ac-a702-4e82b5e17816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data review\n",
    "query = f\"\"\"\n",
    "SELECT Precip_Type\n",
    "FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}`\n",
    "\"\"\"\n",
    "df = bq.query(query = query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d067c3a3-2f28-4a9c-a8b2-9aa35867aaa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rain    85224\n",
       "snow    10712\n",
       "Name: Precip_Type, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Precip_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921b779-59b1-4b36-b037-b600c44511d7",
   "metadata": {},
   "source": [
    "Additional preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05513c29-aac9-4ff6-bf8f-0c60d6993b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignUniqueNumber(df):\n",
    "  map = {'rain': 0, 'snow': 1}\n",
    "  labelLowered = df['Precip_Type'].values\n",
    "  labelNum = []\n",
    "  for label in labelLowered:\n",
    "    labelNum.append(map[label])\n",
    "  df['label'] = labelNum\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e111ffd-ca37-4929-bb78-017c792bdf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (95936,)\n"
     ]
    }
   ],
   "source": [
    "df = assignUniqueNumber(df)\n",
    "y = df.label\n",
    "print('Shape of label tensor:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef930aef-c87b-45d9-bbe0-54f2e4d86bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precip_Type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>snow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>snow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>snow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>snow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95931</th>\n",
       "      <td>rain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95932</th>\n",
       "      <td>rain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95933</th>\n",
       "      <td>rain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95934</th>\n",
       "      <td>rain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95935</th>\n",
       "      <td>rain</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95936 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Precip_Type  label\n",
       "0            snow      1\n",
       "1            snow      1\n",
       "2            snow      1\n",
       "3            snow      1\n",
       "4            snow      1\n",
       "...           ...    ...\n",
       "95931        rain      0\n",
       "95932        rain      0\n",
       "95933        rain      0\n",
       "95934        rain      0\n",
       "95935        rain      0\n",
       "\n",
       "[95936 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f88da202-50c6-4d60-a37c-074125627b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precip_Type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>snow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>snow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>snow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>snow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Precip_Type  label\n",
       "0        snow      1\n",
       "1        snow      1\n",
       "2        snow      1\n",
       "3        snow      1\n",
       "4        snow      1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1ad4ab4-d0a0-42aa-9e78-8a428318b212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-gbq\n",
      "  Downloading pandas_gbq-0.19.2-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from pandas-gbq) (68.2.2)\n",
      "Requirement already satisfied: db-dtypes<2.0.0,>=1.0.4 in /opt/conda/lib/python3.9/site-packages (from pandas-gbq) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.9/site-packages (from pandas-gbq) (1.19.5)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.9/site-packages (from pandas-gbq) (1.4.4)\n",
      "Requirement already satisfied: pyarrow>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from pandas-gbq) (9.0.0)\n",
      "Collecting pydata-google-auth>=1.5.0 (from pandas-gbq)\n",
      "  Obtaining dependency information for pydata-google-auth>=1.5.0 from https://files.pythonhosted.org/packages/28/6b/3320c9ddbfc572108917e8432a07e8bd1e40054d94b5ad40c755afdc1160/pydata_google_auth-1.8.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading pydata_google_auth-1.8.2-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting google-api-core<3.0.0dev,>=2.10.2 (from pandas-gbq)\n",
      "  Obtaining dependency information for google-api-core<3.0.0dev,>=2.10.2 from https://files.pythonhosted.org/packages/4d/ce/4fd62ea66b3508debc795e475336ce915929765870f0ad52328426ba016e/google_api_core-2.12.0-py3-none-any.whl.metadata\n",
      "  Downloading google_api_core-2.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-auth>=2.13.0 (from pandas-gbq)\n",
      "  Obtaining dependency information for google-auth>=2.13.0 from https://files.pythonhosted.org/packages/39/7c/2e4fa55a99f83ef9ef229ac5d59c44ceb90e2d0145711590c0fa39669f32/google_auth-2.23.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.23.3-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib>=0.7.0 (from pandas-gbq)\n",
      "  Obtaining dependency information for google-auth-oauthlib>=0.7.0 from https://files.pythonhosted.org/packages/ce/33/a907b4b67245647746dde8d61e1643ef5d210c88e090d491efd89eff9f95/google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5 in /opt/conda/lib/python3.9/site-packages (from pandas-gbq) (3.11.4)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3.0.0dev,>=2.16.2 in /opt/conda/lib/python3.9/site-packages (from pandas-gbq) (2.16.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.9/site-packages (from db-dtypes<2.0.0,>=1.0.4->pandas-gbq) (23.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.9/site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (1.60.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.9/site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (3.19.6)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.9/site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth>=2.13.0->pandas-gbq) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth>=2.13.0->pandas-gbq) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth>=2.13.0->pandas-gbq) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib>=0.7.0->pandas-gbq) (1.3.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.47.0 in /opt/conda/lib/python3.9/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (1.48.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.9/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (1.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (1.22.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (2.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.9/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.1.4->pandas-gbq) (2023.3.post1)\n",
      "INFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.9/site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (1.48.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.9/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.9/site-packages (from grpcio<2.0dev,>=1.47.0->google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas-gbq) (1.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.13.0->pandas-gbq) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core<3.0.0dev,>=2.10.2->pandas-gbq) (2023.7.22)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.7.0->pandas-gbq) (3.2.2)\n",
      "Downloading google_api_core-2.12.0-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading pydata_google_auth-1.8.2-py2.py3-none-any.whl (15 kB)\n",
      "Installing collected packages: google-auth, google-auth-oauthlib, google-api-core, pydata-google-auth, pandas-gbq\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 1.35.0\n",
      "    Uninstalling google-auth-1.35.0:\n",
      "      Successfully uninstalled google-auth-1.35.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 0.4.6\n",
      "    Uninstalling google-auth-oauthlib-0.4.6:\n",
      "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 1.34.0\n",
      "    Uninstalling google-api-core-1.34.0:\n",
      "      Successfully uninstalled google-api-core-1.34.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.12.0 which is incompatible.\n",
      "google-cloud-pubsub 2.18.4 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.48.0 which is incompatible.\n",
      "tensorboard 2.6.0 requires google-auth<2,>=1.6.3, but you have google-auth 2.23.3 which is incompatible.\n",
      "tensorboard 2.6.0 requires google-auth-oauthlib<0.5,>=0.4.1, but you have google-auth-oauthlib 1.1.0 which is incompatible.\n",
      "tensorflow-serving-api 2.6.5 requires tensorflow<3,>=2.6.5, but you have tensorflow 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-core-2.11.1 google-auth-2.23.0 google-auth-oauthlib-1.1.0 pandas-gbq-0.19.2 pydata-google-auth-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas-gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a6a3570-a0e8-41e0-9df9-3eaccbb97a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ffc05297-0a82-40ab-8bb6-80c9d6b48c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io import gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "621d774d-486a-4c2b-8c80-5eb35b11c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data review\n",
    "query = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}`\n",
    "\"\"\"\n",
    "data = gbq.read_gbq(query, project_id=BQ_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f1c1aaa-5f94-4b3f-adb9-ccc5f3b42b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Drop the rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "categorical_data = encoder.fit_transform(data[categorical_columns])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = encoder.categories_\n",
    "\n",
    "# Create a DataFrame from the categorical data\n",
    "categorical_df = pd.DataFrame(categorical_data, columns=feature_names)\n",
    "\n",
    "# Concatenate the original DataFrame with the new `categorical_df`\n",
    "data = pd.concat([data, categorical_df], axis=1)\n",
    "\n",
    "# Normalize continuous variables\n",
    "scaler = StandardScaler()\n",
    "continuous_columns = ['Temperature__C_', 'Humidity', 'Apparent_Temperature__C_', 'Wind_Speed__km_h_', 'Wind_Bearing__degrees_', 'Pressure__millibars_']  \n",
    "data[continuous_columns] = scaler.fit_transform(data[continuous_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "68957037-a249-423b-bf95-7cdf6cf3c08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Formatted_Date</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Precip_Type</th>\n",
       "      <th>Temperature__C_</th>\n",
       "      <th>Apparent_Temperature__C_</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind_Speed__km_h_</th>\n",
       "      <th>Wind_Bearing__degrees_</th>\n",
       "      <th>Visibility__km_</th>\n",
       "      <th>Loud_Cover</th>\n",
       "      <th>Pressure__millibars_</th>\n",
       "      <th>Daily_Summary</th>\n",
       "      <th>row_ID</th>\n",
       "      <th>splits</th>\n",
       "      <th>(rain,)</th>\n",
       "      <th>(snow,)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-03-21 14:00:00+00:00</td>\n",
       "      <td>Dry</td>\n",
       "      <td>rain</td>\n",
       "      <td>0.842059</td>\n",
       "      <td>0.852554</td>\n",
       "      <td>-2.681548</td>\n",
       "      <td>0.532471</td>\n",
       "      <td>1.233706</td>\n",
       "      <td>9.982</td>\n",
       "      <td>0</td>\n",
       "      <td>0.247705</td>\n",
       "      <td>Clear throughout the day.</td>\n",
       "      <td>4944af1e-4627-44b4-ba42-a4e99ade82bc</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-03-17 16:00:00+00:00</td>\n",
       "      <td>Dry</td>\n",
       "      <td>rain</td>\n",
       "      <td>0.842059</td>\n",
       "      <td>0.852554</td>\n",
       "      <td>-2.528270</td>\n",
       "      <td>0.299835</td>\n",
       "      <td>0.023106</td>\n",
       "      <td>9.982</td>\n",
       "      <td>0</td>\n",
       "      <td>0.153057</td>\n",
       "      <td>Partly cloudy in the morning.</td>\n",
       "      <td>b183c577-863c-4025-8436-6c50b50afc18</td>\n",
       "      <td>TEST</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-03-17 10:00:00+00:00</td>\n",
       "      <td>Dry</td>\n",
       "      <td>rain</td>\n",
       "      <td>0.842059</td>\n",
       "      <td>0.852554</td>\n",
       "      <td>-2.426085</td>\n",
       "      <td>0.532471</td>\n",
       "      <td>0.116229</td>\n",
       "      <td>9.982</td>\n",
       "      <td>0</td>\n",
       "      <td>0.188017</td>\n",
       "      <td>Partly cloudy in the morning.</td>\n",
       "      <td>2f6f438e-2e75-464c-a1e9-fa827f3f874e</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-08-20 08:00:00+00:00</td>\n",
       "      <td>Dry</td>\n",
       "      <td>rain</td>\n",
       "      <td>1.886923</td>\n",
       "      <td>1.662218</td>\n",
       "      <td>-2.272808</td>\n",
       "      <td>-0.912197</td>\n",
       "      <td>1.559636</td>\n",
       "      <td>9.982</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166529</td>\n",
       "      <td>Clear throughout the day.</td>\n",
       "      <td>aa31c8ae-7a6d-4e89-b05b-fce6d303e122</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-07-22 11:00:00+00:00</td>\n",
       "      <td>Dry</td>\n",
       "      <td>rain</td>\n",
       "      <td>2.801179</td>\n",
       "      <td>2.403459</td>\n",
       "      <td>-2.988104</td>\n",
       "      <td>1.600268</td>\n",
       "      <td>0.395598</td>\n",
       "      <td>9.982</td>\n",
       "      <td>0</td>\n",
       "      <td>0.047665</td>\n",
       "      <td>Partly cloudy starting in the afternoon.</td>\n",
       "      <td>8051d38c-4391-4eee-addb-b3b84ce185f5</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Formatted_Date Summary Precip_Type  Temperature__C_  \\\n",
       "0 2012-03-21 14:00:00+00:00     Dry        rain         0.842059   \n",
       "1 2012-03-17 16:00:00+00:00     Dry        rain         0.842059   \n",
       "2 2012-03-17 10:00:00+00:00     Dry        rain         0.842059   \n",
       "3 2012-08-20 08:00:00+00:00     Dry        rain         1.886923   \n",
       "4 2007-07-22 11:00:00+00:00     Dry        rain         2.801179   \n",
       "\n",
       "   Apparent_Temperature__C_  Humidity  Wind_Speed__km_h_  \\\n",
       "0                  0.852554 -2.681548           0.532471   \n",
       "1                  0.852554 -2.528270           0.299835   \n",
       "2                  0.852554 -2.426085           0.532471   \n",
       "3                  1.662218 -2.272808          -0.912197   \n",
       "4                  2.403459 -2.988104           1.600268   \n",
       "\n",
       "   Wind_Bearing__degrees_  Visibility__km_  Loud_Cover  Pressure__millibars_  \\\n",
       "0                1.233706            9.982           0              0.247705   \n",
       "1                0.023106            9.982           0              0.153057   \n",
       "2                0.116229            9.982           0              0.188017   \n",
       "3                1.559636            9.982           0              0.166529   \n",
       "4                0.395598            9.982           0              0.047665   \n",
       "\n",
       "                              Daily_Summary  \\\n",
       "0                 Clear throughout the day.   \n",
       "1             Partly cloudy in the morning.   \n",
       "2             Partly cloudy in the morning.   \n",
       "3                 Clear throughout the day.   \n",
       "4  Partly cloudy starting in the afternoon.   \n",
       "\n",
       "                                 row_ID splits  (rain,)  (snow,)  \n",
       "0  4944af1e-4627-44b4-ba42-a4e99ade82bc  TRAIN      1.0      0.0  \n",
       "1  b183c577-863c-4025-8436-6c50b50afc18   TEST      1.0      0.0  \n",
       "2  2f6f438e-2e75-464c-a1e9-fa827f3f874e  TRAIN      1.0      0.0  \n",
       "3  aa31c8ae-7a6d-4e89-b05b-fce6d303e122  TRAIN      1.0      0.0  \n",
       "4  8051d38c-4391-4eee-addb-b3b84ce185f5  TRAIN      1.0      0.0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "99ccc4d5-a7cc-468b-9e24-f09ebc91b18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95936 entries, 0 to 95935\n",
      "Data columns (total 16 columns):\n",
      " #   Column                    Non-Null Count  Dtype              \n",
      "---  ------                    --------------  -----              \n",
      " 0   Formatted_Date            95936 non-null  datetime64[ns, UTC]\n",
      " 1   Summary                   95936 non-null  object             \n",
      " 2   Precip_Type               95936 non-null  object             \n",
      " 3   Temperature__C_           95936 non-null  float64            \n",
      " 4   Apparent_Temperature__C_  95936 non-null  float64            \n",
      " 5   Humidity                  95936 non-null  float64            \n",
      " 6   Wind_Speed__km_h_         95936 non-null  float64            \n",
      " 7   Wind_Bearing__degrees_    95936 non-null  float64            \n",
      " 8   Visibility__km_           95936 non-null  float64            \n",
      " 9   Loud_Cover                95936 non-null  Int64              \n",
      " 10  Pressure__millibars_      95936 non-null  float64            \n",
      " 11  Daily_Summary             95936 non-null  object             \n",
      " 12  row_ID                    95936 non-null  object             \n",
      " 13  splits                    95936 non-null  object             \n",
      " 14  (rain,)                   95936 non-null  float64            \n",
      " 15  (snow,)                   95936 non-null  float64            \n",
      "dtypes: Int64(1), datetime64[ns, UTC](1), float64(9), object(5)\n",
      "memory usage: 11.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7636cee3-fe35-4764-a557-8bf38ec005b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.6.0\n",
      "  Downloading tensorflow-2.6.0-cp39-cp39-manylinux2010_x86_64.whl (458.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.4/458.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (1.19.5)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (0.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (1.6.3)\n",
      "Requirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (5.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (3.19.6)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (1.1.0)\n",
      "Collecting typing-extensions~=3.7.4 (from tensorflow==2.6.0)\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (0.41.2)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (2.6.0)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (2.6.0)\n",
      "Requirement already satisfied: keras~=2.6 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (2.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow==2.6.0) (1.48.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (68.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow==2.6.0) (2.3.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.0) (6.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.0) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=0.11.15->tensorboard~=2.6->tensorflow==2.6.0) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.0) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.0) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.0) (3.2.2)\n",
      "Installing collected packages: typing-extensions, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.6.5\n",
      "    Uninstalling tensorflow-2.6.5:\n",
      "      Successfully uninstalled tensorflow-2.6.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires grpcio!=1.48.0,<2,>=1.33.1, but you have grpcio 1.48.0 which is incompatible.\n",
      "async-lru 2.0.4 requires typing-extensions>=4.0.0; python_version < \"3.11\", but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "starlette 0.27.0 requires typing-extensions>=3.10.0; python_version < \"3.10\", but you have typing-extensions 3.7.4.3 which is incompatible.\n",
      "tensorflow-serving-api 2.6.5 requires tensorflow<3,>=2.6.5, but you have tensorflow 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tensorflow-2.6.0 typing-extensions-3.7.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.6.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e492f8f7-ba48-43a2-a45a-030a9b1c8f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "import pkg_resources\n",
    "from IPython.display import Markdown as md\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd84c342-0463-4cba-a6e9-e61004dfe1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project = PROJECT_ID, location = REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a629083-a243-464b-a158-b743ac6f1561",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf8cbccc-997b-4fd5-9a16-87692d72a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = f\"gs://{BUCKET}/{SERIES}/{EXPERIMENT}\"\n",
    "DIR = f\"temp/{EXPERIMENT}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe0442a8-1e95-446d-8673-0fc24c21ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {DIR}\n",
    "!mkdir -p {DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b4fb77c-82c5-4006-98c0-33f7ba7d6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMEWORK = 'tf'\n",
    "TASK = 'classification'\n",
    "MODEL_TYPE = 'dnn'\n",
    "EXPERIMENT_NAME = f'experiment-{SERIES}-{EXPERIMENT}-{FRAMEWORK}-{TASK}-{MODEL_TYPE}'\n",
    "RUN_NAME = f'run-{TIMESTAMP}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c34b8b-3a91-4c3f-9074-bec116129217",
   "metadata": {},
   "source": [
    "Get Vertex AI Experiments Tensorboard Instance Name\n",
    "Vertex AI Experiments has managed Tensorboard instances that you can track Tensorboard Experiments (a training run or hyperparameter tuning sweep).\n",
    "\n",
    "The training job will show up as an experiment for the Tensorboard instance and have the same name as the training job ID.\n",
    "\n",
    "This code checks to see if a Tensorboard Instance has been created in the project, retrieves it if so, creates it otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daf85f35-f7cd-4f9f-aaca-124944a88871",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = aiplatform.Tensorboard.list(filter=f\"labels.series={SERIES}\")\n",
    "if tb:\n",
    "    tb = tb[0]\n",
    "else: \n",
    "    tb = aiplatform.Tensorboard.create(display_name = SERIES, labels = {'series' : f'{SERIES}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc871913-ded1-4e84-92ae-ed3c30b088ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/21381076265/locations/us-east1/tensorboards/7569565811191316480'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb.resource_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21421d5e-6b25-4178-a783-cdedb24b7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(experiment = EXPERIMENT_NAME, experiment_tensorboard = tb.resource_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2f6de9c-8329-4ea9-a124-066db7bc0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_PATH = './code/train.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a87d12a1-a632-47f4-8e95-616e24addbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62c92e60-fdd1-4ec1-9463-af11d2fe07c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the code directory\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('code'):\n",
    "    print('The code directory alredy exists')\n",
    "else:\n",
    "    print('Creating the code directory')\n",
    "    os.makedirs('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5ece765-bb98-4768-bf18-cd10ff5c777d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SCRIPT_PATH}\n",
    "\n",
    "# package import\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "import argparse\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2af78c5d-2539-464b-b3a7-1477b9ca4994",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# import argument to local variables\n",
    "parser = argparse.ArgumentParser()\n",
    "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
    "parser.add_argument('--epochs', dest = 'epochs', default = 10, type = int, help = 'Number of Epochs')\n",
    "parser.add_argument('--batch_size', dest = 'batch_size', default = 32, type = int, help = 'Batch Size')\n",
    "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
    "parser.add_argument('--var_omit', dest = 'var_omit', type=str, nargs='*')\n",
    "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
    "parser.add_argument('--bq_project', dest = 'bq_project', type=str)\n",
    "parser.add_argument('--bq_dataset', dest = 'bq_dataset', type=str)\n",
    "parser.add_argument('--bq_table', dest = 'bq_table', type=str)\n",
    "parser.add_argument('--region', dest = 'region', type=str)\n",
    "parser.add_argument('--experiment', dest = 'experiment', type=str)\n",
    "parser.add_argument('--series', dest = 'series', type=str)\n",
    "parser.add_argument('--experiment_name', dest = 'experiment_name', type=str)\n",
    "parser.add_argument('--run_name', dest = 'run_name', type=str)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e40604db-7748-4150-9eef-2262b3d42728",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# clients\n",
    "bq = bigquery.Client(project = args.project_id)\n",
    "aiplatform.init(project = args.project_id, location = args.region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61381f16-902a-48a6-adb6-22736614a741",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# Vertex AI Experiment run setup\n",
    "if args.run_name in [run.name for run in aiplatform.ExperimentRun.list(experiment = args.experiment_name)]:\n",
    "    expRun = aiplatform.ExperimentRun(run_name = args.run_name, experiment = args.experiment_name)\n",
    "else:\n",
    "    expRun = aiplatform.ExperimentRun.create(run_name = args.run_name, experiment = args.experiment_name)\n",
    "expRun.log_params({'experiment': args.experiment, 'series': args.series, 'project_id': args.project_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ceb9e4fb-3717-4de9-8079-40e5f5d6112d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# get schema from bigquery source\n",
    "query = f\"SELECT * FROM {args.bq_project}.{args.bq_dataset}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{args.bq_table}'\"\n",
    "schema = bq.query(query).to_dataframe()\n",
    "\n",
    "# get number of classes from bigquery source\n",
    "nclasses = bq.query(query = f'SELECT DISTINCT {args.var_target} FROM {args.bq_project}.{args.bq_dataset}.{args.bq_table} WHERE {args.var_target} is not null').to_dataframe()\n",
    "nclasses = nclasses.shape[0]\n",
    "expRun.log_params({'data_source': f'bq://{args.bq_project}.{args.bq_dataset}.{args.bq_table}', 'nclasses': nclasses, 'var_split': 'splits', 'var_target': args.var_target})\n",
    "\n",
    "# Make a list of columns to omit\n",
    "OMIT = args.var_omit + ['splits']\n",
    "\n",
    "# use schema to prepare a list of columns to read from BigQuery\n",
    "selected_fields = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
    "\n",
    "# all the columns in this data source are either float64 or int64\n",
    "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in schema[~schema.column_name.isin(OMIT)].data_type.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c60624bd-b02c-4c5c-9604-f06c52ddacf7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# remap input data to Tensorflow inputs of features and target\n",
    "def transTable(row_dict):\n",
    "    target = row_dict.pop(args.var_target)\n",
    "    target = tf.one_hot(tf.cast(target, tf.int64), nclasses)\n",
    "    target = tf.cast(target, tf.float32)\n",
    "    return(row_dict, target)\n",
    "\n",
    "# function to setup a bigquery reader with Tensorflow I/O\n",
    "def bq_reader(split):\n",
    "    reader = BigQueryClient()\n",
    "\n",
    "    training = reader.read_session(\n",
    "        parent = f\"projects/{args.project_id}\",\n",
    "        project_id = args.bq_project,\n",
    "        table_id = args.bq_table,\n",
    "        dataset_id = args.bq_dataset,\n",
    "        selected_fields = selected_fields,\n",
    "        output_types = output_types,\n",
    "        row_restriction = f\"splits='{split}'\",\n",
    "        requested_streams = 3\n",
    "    )\n",
    "    \n",
    "    return training\n",
    "\n",
    "# setup feed for train, validate and test\n",
    "train = bq_reader('TRAIN').parallel_read_rows().prefetch(1).map(transTable).shuffle(args.batch_size*10).batch(args.batch_size)\n",
    "validate = bq_reader('VALIDATE').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
    "test = bq_reader('TEST').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
    "expRun.log_params({'training.batch_size': args.batch_size, 'training.shuffle': 10*args.batch_size, 'training.prefetch': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "27eec047-6592-4b94-ab7a-3be45e41d13a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "# model input definitions\n",
    "feature_columns = {header: tf.feature_column.numeric_column(header) for header in selected_fields if header != args.var_target}\n",
    "feature_layer_inputs = {header: tf.keras.layers.Input(shape = (1,), name = header) for header in selected_fields if header != args.var_target}\n",
    "\n",
    "# feature columns to a Dense Feature Layer\n",
    "feature_layer_outputs = tf.keras.layers.DenseFeatures(feature_columns.values(), name = 'feature_layer')(feature_layer_inputs)\n",
    "\n",
    "# batch normalization of inputs\n",
    "normalized = tf.keras.layers.BatchNormalization(name = 'batch_normalization_layer')(feature_layer_outputs)\n",
    "\n",
    "# logistic - using softmax activation to nclasses\n",
    "logistic = tf.keras.layers.Dense(nclasses, activation = tf.nn.softmax, name = 'logistic')(normalized)\n",
    "\n",
    "# the model\n",
    "model = tf.keras.Model(\n",
    "    inputs = feature_layer_inputs,\n",
    "    outputs = logistic,\n",
    "    name = args.experiment\n",
    ")\n",
    "\n",
    "# compile\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.SGD(), #SGD or Adam\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(curve = 'PR', name = 'auprc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e7d01f6-b37e-4e68-945a-09b057dc04a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# setup tensorboard logs and train\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], histogram_freq=1)\n",
    "history = model.fit(train, epochs = args.epochs, callbacks = [tensorboard_callback], validation_data = validate)\n",
    "expRun.log_params({'training.epochs': history.params['epochs']})\n",
    "for e in range(0, history.params['epochs']):\n",
    "    expRun.log_time_series_metrics(\n",
    "        {\n",
    "            'train_loss': history.history['loss'][e],\n",
    "            'train_accuracy': history.history['accuracy'][e],\n",
    "            'train_auprc': history.history['auprc'][e],\n",
    "            'val_loss': history.history['val_loss'][e],\n",
    "            'val_accuracy': history.history['val_accuracy'][e],\n",
    "            'val_auprc': history.history['val_auprc'][e]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "07e4aaa5-1e88-44a1-9de8-657b7df044bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# test evaluations:\n",
    "loss, accuracy, auprc = model.evaluate(test)\n",
    "expRun.log_metrics({'test_loss': loss, 'test_accuracy': accuracy, 'test_auprc': auprc})\n",
    "\n",
    "# val evaluations:\n",
    "loss, accuracy, auprc = model.evaluate(validate)\n",
    "expRun.log_metrics({'val_loss': loss, 'val_accuracy': accuracy, 'val_auprc': auprc})\n",
    "\n",
    "# training evaluations:\n",
    "loss, accuracy, auprc = model.evaluate(train)\n",
    "expRun.log_metrics({'train_loss': loss, 'train_accuracy': accuracy, 'train_auprc': auprc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c7ba505d-a1a4-4874-a9b0-7da8208ef3be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a {SCRIPT_PATH}\n",
    "\n",
    "# output the model save files\n",
    "model.save(os.getenv(\"AIP_MODEL_DIR\"))\n",
    "expRun.log_params({'model.save': os.getenv(\"AIP_MODEL_DIR\")})\n",
    "expRun.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d294dad0-668c-4fd7-81a7-eaea3309556e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "\n",
       "\n",
       "# package import\n",
       "from tensorflow.python.framework import dtypes\n",
       "from tensorflow_io.bigquery import BigQueryClient\n",
       "import tensorflow as tf\n",
       "from google.cloud import bigquery\n",
       "from google.cloud import aiplatform\n",
       "import argparse\n",
       "import os\n",
       "import sys\n",
       "\n",
       "# import argument to local variables\n",
       "parser = argparse.ArgumentParser()\n",
       "# the passed param, dest: a name for the param, default: if absent fetch this param from the OS, type: type to convert to, help: description of argument\n",
       "parser.add_argument('--epochs', dest = 'epochs', default = 10, type = int, help = 'Number of Epochs')\n",
       "parser.add_argument('--batch_size', dest = 'batch_size', default = 32, type = int, help = 'Batch Size')\n",
       "parser.add_argument('--var_target', dest = 'var_target', type=str)\n",
       "parser.add_argument('--var_omit', dest = 'var_omit', type=str, nargs='*')\n",
       "parser.add_argument('--project_id', dest = 'project_id', type=str)\n",
       "parser.add_argument('--bq_project', dest = 'bq_project', type=str)\n",
       "parser.add_argument('--bq_dataset', dest = 'bq_dataset', type=str)\n",
       "parser.add_argument('--bq_table', dest = 'bq_table', type=str)\n",
       "parser.add_argument('--region', dest = 'region', type=str)\n",
       "parser.add_argument('--experiment', dest = 'experiment', type=str)\n",
       "parser.add_argument('--series', dest = 'series', type=str)\n",
       "parser.add_argument('--experiment_name', dest = 'experiment_name', type=str)\n",
       "parser.add_argument('--run_name', dest = 'run_name', type=str)\n",
       "args = parser.parse_args()\n",
       "\n",
       "# clients\n",
       "bq = bigquery.Client(project = args.project_id)\n",
       "aiplatform.init(project = args.project_id, location = args.region)\n",
       "\n",
       "# Vertex AI Experiment run setup\n",
       "if args.run_name in [run.name for run in aiplatform.ExperimentRun.list(experiment = args.experiment_name)]:\n",
       "    expRun = aiplatform.ExperimentRun(run_name = args.run_name, experiment = args.experiment_name)\n",
       "else:\n",
       "    expRun = aiplatform.ExperimentRun.create(run_name = args.run_name, experiment = args.experiment_name)\n",
       "expRun.log_params({'experiment': args.experiment, 'series': args.series, 'project_id': args.project_id})\n",
       "\n",
       "# get schema from bigquery source\n",
       "query = f\"SELECT * FROM {args.bq_project}.{args.bq_dataset}.INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{args.bq_table}'\"\n",
       "schema = bq.query(query).to_dataframe()\n",
       "\n",
       "# get number of classes from bigquery source\n",
       "nclasses = bq.query(query = f'SELECT DISTINCT {args.var_target} FROM {args.bq_project}.{args.bq_dataset}.{args.bq_table} WHERE {args.var_target} is not null').to_dataframe()\n",
       "nclasses = nclasses.shape[0]\n",
       "expRun.log_params({'data_source': f'bq://{args.bq_project}.{args.bq_dataset}.{args.bq_table}', 'nclasses': nclasses, 'var_split': 'splits', 'var_target': args.var_target})\n",
       "\n",
       "# Make a list of columns to omit\n",
       "OMIT = args.var_omit + ['splits']\n",
       "\n",
       "# use schema to prepare a list of columns to read from BigQuery\n",
       "selected_fields = schema[~schema.column_name.isin(OMIT)].column_name.tolist()\n",
       "\n",
       "# all the columns in this data source are either float64 or int64\n",
       "output_types = [dtypes.float64 if x=='FLOAT64' else dtypes.int64 for x in schema[~schema.column_name.isin(OMIT)].data_type.tolist()]\n",
       "\n",
       "# remap input data to Tensorflow inputs of features and target\n",
       "def transTable(row_dict):\n",
       "    target = row_dict.pop(args.var_target)\n",
       "    target = tf.one_hot(tf.cast(target, tf.int64), nclasses)\n",
       "    target = tf.cast(target, tf.float32)\n",
       "    return(row_dict, target)\n",
       "\n",
       "# function to setup a bigquery reader with Tensorflow I/O\n",
       "def bq_reader(split):\n",
       "    reader = BigQueryClient()\n",
       "\n",
       "    training = reader.read_session(\n",
       "        parent = f\"projects/{args.project_id}\",\n",
       "        project_id = args.bq_project,\n",
       "        table_id = args.bq_table,\n",
       "        dataset_id = args.bq_dataset,\n",
       "        selected_fields = selected_fields,\n",
       "        output_types = output_types,\n",
       "        row_restriction = f\"splits='{split}'\",\n",
       "        requested_streams = 3\n",
       "    )\n",
       "    \n",
       "    return training\n",
       "\n",
       "# setup feed for train, validate and test\n",
       "train = bq_reader('TRAIN').parallel_read_rows().prefetch(1).map(transTable).shuffle(args.batch_size*10).batch(args.batch_size)\n",
       "validate = bq_reader('VALIDATE').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
       "test = bq_reader('TEST').parallel_read_rows().prefetch(1).map(transTable).batch(args.batch_size)\n",
       "expRun.log_params({'training.batch_size': args.batch_size, 'training.shuffle': 10*args.batch_size, 'training.prefetch': 1})\n",
       "\n",
       "# Logistic Regression\n",
       "\n",
       "# model input definitions\n",
       "feature_columns = {header: tf.feature_column.numeric_column(header) for header in selected_fields if header != args.var_target}\n",
       "feature_layer_inputs = {header: tf.keras.layers.Input(shape = (1,), name = header) for header in selected_fields if header != args.var_target}\n",
       "\n",
       "# feature columns to a Dense Feature Layer\n",
       "feature_layer_outputs = tf.keras.layers.DenseFeatures(feature_columns.values(), name = 'feature_layer')(feature_layer_inputs)\n",
       "\n",
       "# batch normalization of inputs\n",
       "normalized = tf.keras.layers.BatchNormalization(name = 'batch_normalization_layer')(feature_layer_outputs)\n",
       "\n",
       "# logistic - using softmax activation to nclasses\n",
       "logistic = tf.keras.layers.Dense(nclasses, activation = tf.nn.softmax, name = 'logistic')(normalized)\n",
       "\n",
       "# the model\n",
       "model = tf.keras.Model(\n",
       "    inputs = feature_layer_inputs,\n",
       "    outputs = logistic,\n",
       "    name = args.experiment\n",
       ")\n",
       "\n",
       "# compile\n",
       "model.compile(\n",
       "    optimizer = tf.keras.optimizers.SGD(), #SGD or Adam\n",
       "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
       "    metrics = ['accuracy', tf.keras.metrics.AUC(curve = 'PR', name = 'auprc')]\n",
       "\n",
       "# setup tensorboard logs and train\n",
       "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'], histogram_freq=1)\n",
       "history = model.fit(train, epochs = args.epochs, callbacks = [tensorboard_callback], validation_data = validate)\n",
       "expRun.log_params({'training.epochs': history.params['epochs']})\n",
       "for e in range(0, history.params['epochs']):\n",
       "    expRun.log_time_series_metrics(\n",
       "        {\n",
       "            'train_loss': history.history['loss'][e],\n",
       "            'train_accuracy': history.history['accuracy'][e],\n",
       "            'train_auprc': history.history['auprc'][e],\n",
       "            'val_loss': history.history['val_loss'][e],\n",
       "            'val_accuracy': history.history['val_accuracy'][e],\n",
       "            'val_auprc': history.history['val_auprc'][e]\n",
       "        }\n",
       "    )\n",
       "\n",
       "# test evaluations:\n",
       "loss, accuracy, auprc = model.evaluate(test)\n",
       "expRun.log_metrics({'test_loss': loss, 'test_accuracy': accuracy, 'test_auprc': auprc})\n",
       "\n",
       "# val evaluations:\n",
       "loss, accuracy, auprc = model.evaluate(validate)\n",
       "expRun.log_metrics({'val_loss': loss, 'val_accuracy': accuracy, 'val_auprc': auprc})\n",
       "\n",
       "# training evaluations:\n",
       "loss, accuracy, auprc = model.evaluate(train)\n",
       "expRun.log_metrics({'train_loss': loss, 'train_accuracy': accuracy, 'train_auprc': auprc})\n",
       "\n",
       "# output the model save files\n",
       "model.save(os.getenv(\"AIP_MODEL_DIR\"))\n",
       "expRun.log_params({'model.save': os.getenv(\"AIP_MODEL_DIR\")})\n",
       "expRun.end_run()\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(SCRIPT_PATH, 'r') as file:\n",
    "    d = file.read()\n",
    "md(f\"```python\\n\\n{d}\\n```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19c566f9-3e4f-42b8-898e-f18bfaa77aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 95936 entries, 0 to 95935\n",
      "Data columns (total 16 columns):\n",
      " #   Column                    Non-Null Count  Dtype              \n",
      "---  ------                    --------------  -----              \n",
      " 0   Formatted_Date            95936 non-null  datetime64[ns, UTC]\n",
      " 1   Summary                   95936 non-null  object             \n",
      " 2   Precip_Type               95936 non-null  object             \n",
      " 3   Temperature__C_           95936 non-null  float64            \n",
      " 4   Apparent_Temperature__C_  95936 non-null  float64            \n",
      " 5   Humidity                  95936 non-null  float64            \n",
      " 6   Wind_Speed__km_h_         95936 non-null  float64            \n",
      " 7   Wind_Bearing__degrees_    95936 non-null  float64            \n",
      " 8   Visibility__km_           95936 non-null  float64            \n",
      " 9   Loud_Cover                95936 non-null  Int64              \n",
      " 10  Pressure__millibars_      95936 non-null  float64            \n",
      " 11  Daily_Summary             95936 non-null  object             \n",
      " 12  row_ID                    95936 non-null  object             \n",
      " 13  splits                    95936 non-null  object             \n",
      " 14  (rain,)                   95936 non-null  float64            \n",
      " 15  (snow,)                   95936 non-null  float64            \n",
      "dtypes: Int64(1), datetime64[ns, UTC](1), float64(9), object(5)\n",
      "memory usage: 11.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1e2d8c2e-d084-4d3b-a206-401205f18d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources\n",
    "TRAIN_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-7:latest'\n",
    "DEPLOY_IMAGE ='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-7:latest'\n",
    "TRAIN_COMPUTE = 'n1-standard-4'\n",
    "DEPLOY_COMPUTE = 'n1-standard-4'\n",
    "\n",
    "# Model Training\n",
    "VAR_TARGET = 'Precip_Type'\n",
    "VAR_OMIT = 'row_ID','Summary', 'Formatted_Date', 'Loud_Cover', 'Daily_Summary', 'splits'\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "385f23fa-60c5-469f-ba19-fea72ca13e75",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"tuple\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m CMDARGS \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--epochs=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(EPOCHS),\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--batch_size=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(BATCH_SIZE),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--var_target=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m VAR_TARGET,\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m--var_omit=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVAR_OMIT\u001b[49m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--project_id=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m PROJECT_ID,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--bq_project=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m BQ_PROJECT,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--bq_dataset=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m BQ_DATASET,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--bq_table=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m BQ_TABLE,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--region=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m REGION,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--experiment=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m EXPERIMENT,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--series=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m SERIES,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--experiment_name=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m EXPERIMENT_NAME,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--run_name=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m RUN_NAME\n\u001b[1;32m     15\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"tuple\") to str"
     ]
    }
   ],
   "source": [
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--batch_size=\" + str(BATCH_SIZE),\n",
    "    \"--var_target=\" + VAR_TARGET,\n",
    "    \"--var_omit=\" + VAR_OMIT,\n",
    "    \"--project_id=\" + PROJECT_ID,\n",
    "    \"--bq_project=\" + BQ_PROJECT,\n",
    "    \"--bq_dataset=\" + BQ_DATASET,\n",
    "    \"--bq_table=\" + BQ_TABLE,\n",
    "    \"--region=\" + REGION,\n",
    "    \"--experiment=\" + EXPERIMENT,\n",
    "    \"--series=\" + SERIES,\n",
    "    \"--experiment_name=\" + EXPERIMENT_NAME,\n",
    "    \"--run_name=\" + RUN_NAME\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6fb844-94b3-4bd5-8870-21bbf3991a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
